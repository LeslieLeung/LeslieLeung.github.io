[{"url":"/2021/01/09/LAMP服务器环境搭建及thinkphp工程部署/","content":"# LAMP服务器环境搭建及工程部署\n\n这篇文章是我之前讲的环境搭建和项目部署视频讲解的文字版本，由于之前直播录屏的时候没有空闲的服务器进行搭建，所以没有详细的截图等可以展示。\n\n现在刚好我手头有一台vps刚重装完，记录一下搭建的过程。\n\n## 简介\n\nLAMP环境也就是XAMPP的Linux版本，指的是Linux+Apache+Mysql+PHP的组合，是比较经典的建站环境。\n\n之前提到过，部署LAMP环境主要有以下的几种方式：\n\n- 包管理安装（用apt-get、yum等逐个安装Apache、mysql、php）\n  - 优点：包管理能解决一定的依赖、环境问题\n  - 缺点：不可自定义一些特殊功能，还是会存在一定的环境问题\n- 手动编译安装（下载源码编译安装）\n  - 优点：完全可自定义（如worker等）\n  - 缺点：需要解决大量的环境问题才能顺利编译，不利于快速部署\n- 一键安装脚本\n  - 优点：简单快捷，可以选择各个组件的版本，适合新手\n  - 缺点：有自定义组件需求的时候比较麻烦，安全问题（夹带私货、挖矿）\n\n一般学习和普通生产用途基本可以无脑选一键安装脚本，我现在使用的这个脚本（https://lamp.sh/）我自己已经使用了好几年，一直比较稳定，提供的帮助也挺好，基本按部就班做就可以成功搭建。\n\n同时还有一个LNMP（Nginx）的一键安装包（https://lnmp.org/），也很好用，我后续演示的腾讯云中使用的就是LNMP。两个的差异主要是http服务器分别是Apache和Nginx，这两个的差异大家可以自行研究。\n\n\n\n## 上手\n\n安装推荐使用交互安装方式（https://lamp.sh/install.html），可以看到\n\n> **系统需求**\n>\n> - 系统支持：Amazon Linux 2018.03/Fedora 29/CentOS 6+/Debian 8+/Ubuntu 14+\n> - 内存要求：≥ 512MB\n> - 硬盘要求：至少 **5GB** 以上的剩余空间\n> - 服务器必须配置好 **软件源** 和 **可连接外网**\n> - 必须具有系统 root 权限\n> - **强烈建议**使用**全新系统**来安装\n>\n> **支持组件**\n>\n> - 支持 PHP 自带**几乎所有**组件\n> - 支持 SQLite、MySQL、MariaDB、Percona Server 数据库\n> - 支持 Yaf（可选安装）\n> - 支持 Redis（可选安装）\n> - 支持 XCache （可选安装）\n> - 支持 Swoole （可选安装）\n> - 支持 Memcached （可选安装）\n> - 支持 ImageMagick （可选安装）\n> - 支持 GraphicsMagick （可选安装）\n> - 支持 ionCube Loader （可选安装）\n> - 自助升级 Apache，PHP，phpMyAdmin，Adminer，MySQL/MariaDB/Percona Server至最新版本\n> - 命令行新增虚拟主机（使用 lamp 命令），操作简便\n> - 支持一键卸载程序（不会删除网站数据）\n>\n\n系统需求方面，如果你使用的是云服务器，基本都能达到使用这个脚本的最低要求（如果达不到，使用编译的方法安装估计也够呛）。同时开发常用的组件（redis、memcached）这些手动安装可能会遇到比较多环境问题的也可以一次性配好，很方便。\n\n\n\n下面开始安装。这里默认读者已经学会使用**ssh**连接到服务器并熟悉一些基本**Linux命令**的操作。其实整个过程只需要按照上面链接的指示复制粘贴命令即可。\n\n### 我的环境\n\n```\nBandwagonHost SPECIAL 10G KVM PROMO V3 - LOS ANGELES - CN2\n1C1T 512M\nOS:CentOS7\n```\n\n### 事前\n\n安装脚本运行所需要的几个依赖\n\n```bash\nyum -y install wget screen git      // for Amazon Linux/CentOS/Fedora\napt-get -y install wget screen git  // for Debian/Ubuntu\n```\n\n我的环境是CentOS，所以选择上面的yum命令执行。\n\n![](http://img.ameow.xyz/20200327023852.png)\n\n然后下载脚本、赋予权限\n\n![](http://img.ameow.xyz/20200327023851.png)\n\n### 动手\n\n接下来就可以开始安装了，这里也可以看到自己服务器的配置信息\n\n![](http://img.ameow.xyz/20200327024121.png)\n\n接下来就是一连串的选择了，Apache这里选择1或者直接回车（只有这个了）。httpd是Apache服务器的别名。\n\n![](http://img.ameow.xyz/20200327024337.png)\n\n这里可以选择一些apache的module，这是脚本安装唯一的自定义性。用不到，所以继续回车默认不安装。\n\n![](http://img.ameow.xyz/20200327024559.png)\n\n接下来这里可以选择mysql和php的版本，mysql建议选5.7，8.0刚出稳定性和社区支持还没到位，php也是选择默认的7.3即可。注意mysql可以自定义数据路径，如果有需求可以自定义，一般直接默认一路回车即可。**注意！！！这里设置的mysql root password一定要记住！**\n\n![](http://img.ameow.xyz/20200327024910.png)\n\n接下来有PHP组件和PHPmyadmin可选，我的项目用到了redis和memcached，所以两个都选择上。phpmyadmin则选择默认即可。\n\n![](http://img.ameow.xyz/20200327025043.png)\n\n最后一步还有一个Kod什么的可选，我用不到，就不装。然后到这个界面，还可以看到一次mysql 的Root Password，一定要记住。看过没有问题，就可以按任意键开始安装。接下来就是一段漫长的滚代码，时间取决于你服务器的性能和网络环境等。现在就可以开始挂一会儿机了。到最后会提示安装完成。\n\n> 最后这台机器因为内存不足（可以看到上面只有<512M）所以没有办法完成php的编译安装，所以后面的内容我会使用已经安装好LAMP环境的另一台腾讯云演示。\n\n> KodExplorer后面发现是一个商业软件，类似云盘之类的东西，应该是作者的恰饭，有需要可以看看，建议选择不要安装。\n\n### 虚拟主机添加（可选）\n\n> 1. 安装 LAMP 环境\n> 2. 添加虚拟主机。运行命令：lamp add\n>    （如果只建一个站，则可以直接将网站程序上传至 /data/www/default 目录下即可）\n> 3. 上传并解压网站程序到网站目录，默认位置为：/data/www/网站域名/\n> 4. 更改网站目录权限。以 root 用户登录，运行：chown -R apache:apache /data/www/网站域名/\n> 5. 运行网站安装程序完成网站安装\n\n这里虚拟主机的意思是你可以绑定多个域名（要先修改域名解析到这个服务器的ip），每个域名各自有一个网站根目录，通过域名直接能访问到不同的网站，但其实还是在同一台服务器上。如果你没有域名，可以直接用ip作为域名。（没必要）\n\n同时，这个脚本里也提供了一键配置https（ssl证书）的功能，只需要上传证书就可以完成配置。\n\n## 工程部署\n\n> **程序目录：**\n>\n> - MySQL 安装目录： /usr/local/mysql\n> - MySQL 数据库目录： /usr/local/mysql/data（默认路径，安装时可更改）\n> - MariaDB 安装目录： /usr/local/mariadb\n> - MariaDB 数据库目录： /usr/local/mariadb/data（默认路径，安装时可更改）\n> - Percona 安装目录： /usr/local/percona\n> - Percona 数据库目录： /usr/local/percona/data（默认路径，安装时可更改）\n> - PHP 安装目录： /usr/local/php\n> - Apache 安装目录： /usr/local/apache\n> - phpMyAdmin 安装目录： /data/www/default/phpmyadmin\n> - KodExplorer 安装目录： /data/www/default/kod\n>\n> **网站目录：**\n>\n> - 默认的网站根目录： /data/www/default\n> - 默认页位置： /data/www/default/index.html\n> - 新建网站默认目录： /data/www/网站域名\n> - phpmyadmin 后台地址： http://域名或IP/phpmyadmin/\n> - phpmyadmin 默认用户名：root 默认密码：root\n>   （**注：**此密码为 MySQL 的 root 密码。在安装时会要求输入，如不输入则为默认密码 root）\n> - XCache 后台地址： http://域名或IP/xcache/\n> - XCache 默认用户名：admin 默认密码：123456\n>   （**注：**用户名和密码在配置文件 /usr/local/php/php.d/xcache.ini 中定义）\n>\n> **配置文件：**\n>\n> - Apache 日志目录： /usr/local/apache/logs\n> - 新建网站日志目录： /data/wwwlog/网站域名\n> - Apache 默认 SSL 配置文件： /usr/local/apache/conf/extra/httpd-ssl.conf\n> - 新建网站配置文件： /usr/local/apache/conf/vhost/网站域名.conf\n> - PHP 配置文件： /usr/local/php/etc/php.ini\n> - PHP 所有扩展配置文件目录: /usr/local/php/php.d/\n> - MySQL 配置文件： /etc/my.cnf\n\n这里有所有的目录的位置。可以看到，默认的网站根目录在`/data/www/default`，这个就相当于win下xampp的`htdocs`文件夹。（如果你配置了虚拟主机，那么根目录在`/data/www/YOUR_DOMAIN_NAME`）\n\n与win下部署方式类似，只需要把php工程文件夹拷贝到网站根目录即可。（题外话：我有前辈以及一些同学习惯直接把项目建在htdocs下，或者整个htdocs就是一个项目，我不提倡这种做法，会对项目管理造成混乱，不是很好的开发习惯。）\n\n传输文件我一般使用PHPstorm的`Deployment`功能或者使用`winscp`进行文件的传输。接下来分别介绍一下。\n\n### winscp\n\n文件传输我习惯用winscp，这个软件十分简洁，就是一个单纯的文件管理器，没有其他太多的功能，通过sftp协议传输文件，不用另外配置。当然如果你有部署ftp服务，也可以使用其他一些ftp软件如FileZilla等进行管理。\n\n进入winscp，配置好连接，然后点击登录即可。\n\n![](http://img.ameow.xyz/20200327135015.png)\n\n服务器端（右边）来到`/data/www/default`(或你的网站根目录，我这里使用的是LNMP，根目录在`/home/wwwroot/xxx`)。本地端（左边）可以打开到相应的项目目录，也可以用文件管理器打开到相应的项目目录。\n\n![](http://img.ameow.xyz/20200327135818.png)\n\n将要部署的项目直接拖过去右边/从文件管理器拖过去右边。\n\n如果是更新文件，则选择全部选是，进行替换。\n\n![](http://img.ameow.xyz/20200327140303.png)\n\n到这里就完成了。\n\n\n\n### Deployment（jetbrain系IDE可用）\n\n首先需要进行一定的配置。同样以这个tutorial项目为例。\n\n在工具栏找到`Tools->Deployment->Configuration...`\n\n![](http://img.ameow.xyz/20200327140600.png)\n\n在弹出的窗口中点击左上角+号，选择sftp，然后输入相应的信息，点击Test Connection，如果成功说明没有问题。\n\n![](http://img.ameow.xyz/20200327140747.png)\n\n接下来点击Mappings选项卡。选择或输入远程的路径，点确认。\n\n![](http://img.ameow.xyz/20200327141014.png)\n\n最后，如果你使用的是thinkphp框架，需要排除掉runtime和vendor文件夹（里面放的是运行时文件，容量比较大，而且环境不同没必要同步）。这里需要注意本地路径需要使用绝对路径。\n\n（这里用了另外一个项目的例子，懒得再写了）\n\n![](http://img.ameow.xyz/20200327141330.png)\n\n配置好后，每次需要上传时，点击里面的upload即可。Sync有代码diff功能，也可以使用这个。\n\n![](http://img.ameow.xyz/20200327141543.png)\n\n\n\n过程到这里就结束了，希望能帮助大家顺利完成部署。"},{"url":"/2021/01/09/RESTful还是JSON-RPC？/","content":"# RESTful还是JSON-RPC？\n\n最近突然被问到：“现在用的都是RESTful的接口吗？”一下子被问得有点蒙， 我确实在很多开发文档里见过REST、RESTful等等字眼，但我自己在用的却好像并不是RESTful的API。那我在用的是什么？与RESTful孰好孰坏？\n\n## 我们在说RESTful和JSON-RPC的时候在讨论什么\n\n首先需要明确的是，RESTful和JSON-RPC都不是指软件库、框架之类的东西，也不是设计模式（与MVC模式等无关），只是一种**代码风格**。\n\n这两个都属于**Web Service模型**，用于帮助人们解决应用程序与服务器**传递数据**的问题，详细细分大致如下：\n\n- SOA模型（面向消息）\n- RPC模型（面向方法）\n  - XML-RPC\n  - JSON-RPC\n  - SOAP+WSDL\n- REST模型（面向资源）\n\n当前而言，比较实际的基本只剩下JSON-RPC与REST两个比较流行的做法。\n\n下面详细讲一下RESTful和JSON-RPC分别是什么，以及涉及到的一些知识点。\n\n## RESTful和JSON-PRC是什么\n\n### RESTful \n\nREST，即Representational State Transfer。总结来说，就是\n\n1. 每一个URI代表一种资源\n2. 客户端和服务器之间，传递这种资源的某种表现层\n3. 客户端通过四个HTTP动词，对服务器端资源进行操作，实现\"表现层状态转化\"\n\n这也是区别于RPC的面向方法，REST是面向资源的。\n\n详细的内容可以参考这两篇文章：[理解RESTful架构](http://www.ruanyifeng.com/blog/2011/09/restful.html)，[RESTful API 设计指南](http://www.ruanyifeng.com/blog/2014/05/restful_api.html)\n\n举例（来源 [WEB开发中，使用JSON-RPC好，还是RESTful API好？ - 邵励治的回答 - 知乎](https://www.zhihu.com/question/28570307/answer/549218509)）\n\n```\n作者：邵励治\n链接：https://www.zhihu.com/question/28570307/answer/549218509\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n\nAPI : Create News\nURL :\n    https://xxx.xxx.xxx.xxx/api/v2/news\nMETHOD : \n    POST\nPARAMETERS : \n    name : String\n    content : String\nRETURN : \n    STATUS : 200 OK\n    JSON :\n    {\n        \"newsId\" : \"xxxxxx\",\n        \"name\" : \"xxxx\",\n        \"content\" : \"xxxx\"\n    }\n\nAPI : Get Single News\nURL : \n    https://xxx.xxx.xxx.xxx/api/v2/news/:newsId\nMETHOD : \n    GET\nRETURN :\n    STATUS : 200 OK\n    JSON :\n    {\n        \"newsId\" : \"xxxxxx\",\n        \"name\" : \"xxxx\",\n        \"content\" : \"xxxx\"\n    }\n\nAPI : Edit News\nURL : \n    https://xxx.xxx.xxx.xxx/api/v2/news/:newsId\nMETHOD : \n    PATCH\nPARAMETERS : \n    name : String\n    content : String\nRETURN :\n    STATUS : 200 OK\n    JSON :\n    {\n        \"newsId\" : \"xxxxxx\",\n        \"name\" : \"xxxx\",\n        \"content\" : \"xxxx\"\n    }\n\nAPI : List News\nURL : \n    https://xxx.xxx.xxx.xxx/api/v2/news\nMETHOD : \n    GET\nRETURN : \n    STATUS : 200 OK\n    JSON : \n    [\n        {\n            \"newsId\" : \"xxxxxx\",\n            \"name\" : \"xxxx\",\n            \"content\" : \"xxxx\"\n        },\n        {\n            \"newsId\" : \"xxxxxx\",\n            \"name\" : \"xxxx\",\n            \"content\" : \"xxxx\"\n        },\n        ......（省略）\n    ]\n```\n\n比较简单地说就是，大家请求一样的url（GET方法有一个例外，url中带了一个id），通过不同的请求方法，分别进行不同的操作（CRUD）。\n\n### JSON-RPC\n\nJSON-RPC是一个无状态且轻量级的远程过程调用（RPC）传送协议，通过JSON传递内容。远程过程调用意思就是，用函数思维写API，用JSON传值，返回一个JSON。\n\n同样举例\n\n```\nAPI : getNewsName\nURL : \n    https://xxx.xxx.xxx.xxx/api/v1/getNewsName\nMETHOD : \n    POST\nPARAMETERS : \n    newsId : String\nRETURN : \n    {\n        \"username\" : \"shaolizhi\"\n    }\n\n作者：邵励治\n链接：https://www.zhihu.com/question/28570307/answer/549218509\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n```\n\n等同于\n\n```text\ngetNewsName( newsId : String ) : String\n```\n\n## 该用哪种？\n\n写这篇文章前，我看了知乎上这样一个问题[WEB开发中，使用JSON-RPC好，还是RESTful API好？](https://www.zhihu.com/question/28570307)，可能有这么一个小规律：前端喜欢REST，后端喜欢JSON-RPC。\n\n下面有一些在看了一些博客、回答以及结合自身开发经验的见解，如有谬误，欢迎指正。\n\n### RESTful的逻辑思维比较难转换\n\n初学coding，比较定式的是函数的思维，考虑的是输入什么、输出什么。而RESTful面向资源的思维在写一些功能的时候就会比较别扭，考虑以下例子：写一个根据uid获取用户信息的接口。\n\n- JSON-RPC：输入用户的uid，输出用户的信息\n- REST：先用POST方法创建一个资源（查询），再GET请求一个资源（刚才创建的查询）\n\n相比之下，REST的一个问题就暴露出来：一定要把所有东西都变成**资源**，然后再要把对应的操作映射成“增删改查”（HTTP的四个方法）。有时候一些很简单的功能，例如一个登录的功能，使用RESTful的话就会演变成十分复杂的问题，化简为繁没必要。\n\n### RESTful 在业务简单、有大量静态资源的情况下有优势\n\n在研究过程中我看到了许多例子，都是用的博客、新闻之类的网站应用举的例子，实际上这些比较共同的特点就是里面有比较多的静态资源，而且业务逻辑相对比较简单，可以用简单的CRUD就能搞定了。那么优势在哪里呢？\n\n例如获取某一篇文章的接口，因为请求某个资源，所以这个接口应该会使用GET方法，在url中直接带上文章的id来获取文章。\n\n这里就需要引入GET和POST的异同了。\n\n|                  |                             GET                              | POST                                                         |\n| :--------------- | :----------------------------------------------------------: | ------------------------------------------------------------ |\n| 后退按钮/刷新    |                             无害                             | 数据会被重新提交（浏览器应该告知用户数据会被重新提交）。     |\n| 书签             |                         可收藏为书签                         | 不可收藏为书签                                               |\n| 缓存             |                           能被缓存                           | 不能缓存                                                     |\n| 编码类型         |              application/x-www-form-urlencoded               | application/x-www-form-urlencoded 或 multipart/form-data。为二进制数据使用多重编码。 |\n| 历史             |                   参数保留在浏览器历史中。                   | 参数不会保存在浏览器历史中。                                 |\n| 对数据长度的限制 | 是的。当发送数据时，GET 方法向 URL 添加数据；URL 的长度是受限制的（URL 的最大长度是 2048 个字符）。 | 无限制。                                                     |\n| 对数据类型的限制 |                     只允许 ASCII 字符。                      | 没有限制。也允许二进制数据。                                 |\n| 安全性           | 与 POST 相比，GET 的安全性较差，因为所发送的数据是 URL 的一部分。在发送密码或其他敏感信息时绝不要使用 GET ！ | POST 比 GET 更安全，因为参数不会被保存在浏览器历史或 web 服务器日志中。 |\n| 可见性           |              数据在 URL 中对所有人都是可见的。               | 数据不会显示在 URL 中。                                      |\n\n\n\n比较显而易见的就是，GET请求可以被缓存。意思是，如果有一个人向web服务器GET请求，那么这个请求会被缓存起来，下一个同样的GET的时候，就会从缓存中取出上一次的返回，在高并发场景下缓存的引入能够有效提高性能。如果使用POST JSON的方法，每次web服务器都需要去调用底下的PHP或者Java之类，会增大开销。\n\n另外一个比较细微的地方是，POST实际需要两次TCP，而GET只需要一次。尽管实践证明两种方法在网络良好时差距不大，但或多或少是一个优化的点。\n\n然而考虑另外一个场景：搜索一篇文章。\n\n首先这里缓存的意义就没有那么大了，每个用户输入的关键词可能都会不一样，之前的缓存优势没有了。然后，如果搜索中有多个条件，有可能会超出URL的上限（URL最大长度2048），还可能因为搜索条件中有中文等特殊字符出现编码等问题，且搜索中不能包含敏感信息之类，比较麻烦。可能有人会说，GET也可以在Body里面放JSON呀！标准来说GET应该是没有Body的，对比RESTful追求的“规范”有一些打脸。另外也没有办法解决上限的问题，可以说POST一个JSON可以**一劳永逸**。\n\n### RESTful在规范、文档编写等比较方便\n\n说实话这个也是见仁见智。在前后端分离开发里，接口文档、接口的结构本来就需要前后端开发比较积极地商量沟通。相对而言，RESTful比较“前端主导”，RPC比较“后端主导”。也有说法是RESTful对外，RPC对内的（这里我对内外的理解是，外指团队、公司外，内指团队内、前后端间）。\n\n但是RESTful更规范化是肯定的，可以参考[RESTful API 设计指南](http://www.ruanyifeng.com/blog/2014/05/restful_api.html)，且有现成的工具可以生成API文档。而JSON-RPC则比较宽松，具体的结构可以有组织、公司的规范，也会有个人的习惯、喜好。\n\n然而在实际开发中我一直使用的JSON-RPC在前后端沟通、文档编写中都比较顺利。这里顺带安利一个API文档工具[DOCWAY](http://www.docway.net/)（以前叫小幺鸡），支持团队协作，简单好用。\n\n### GET？POST？\n\n这个是研究这两个方式的时候衍生出的另一个问题：什么时候用GET？什么时候用POST？\n\n毫无疑问，GET适合获取静态资源，通过QueryString获取参数；而传递JSON、包含敏感信息等应该用POST。但再来看两个我觉得比较经典的、微信开发文档的例子：\n\n[getAccessToken](https://developers.weixin.qq.com/miniprogram/dev/api-backend/open-api/access-token/auth.getAccessToken.html)\n\n![](http://img.ameow.xyz/20200405173643.png)\n\n这是一个获取AccessToken的接口，这里就使用了GET的方法，我觉得原因有以下几个\n\n1. 传递的都是ASCII字符，不受GET方法的限制\n2. appid和secret已经是相对乱码，也没有加密的必要\n3. 接口的功能比较简单，参数固定\n4. 请求的频率会比较高（考虑每次操作都需要这个token，且token比较容易过期）\n\n再看另一个发送消息的接口。\n\n[subscribeMessage.send]([https://developers.weixin.qq.com/miniprogram/dev/api-backend/open-api/subscribe-message/subscribeMessage.send.html#%E4%BA%91%E8%B0%83%E7%94%A8](https://developers.weixin.qq.com/miniprogram/dev/api-backend/open-api/subscribe-message/subscribeMessage.send.html#云调用))\n\n![](http://img.ameow.xyz/20200405174113.png)\n\n这个接口就使用了POST的方法（尽管也有一个参数在url中）。分析如下\n\n1. 需要传递的内容较多，且可能有中文、特殊字符等，可能会超出GET的长度和字符限制\n2. 这里access_token没有放在json中，而是放在了url里。从功能上看这个放在json和url里都没有问题，可能是为了方便开发，服务端可以直接把前端送过来的json数据直接再传递给微信接口\n3. 接口结构比较复杂，而且多变。发送的消息根据模板可以有不一样的结构（每个模板的可以填的空、空的类型之类都不一样）\n4. 请求的频率比较低（不一定，不好做判断）\n\n通过这个例子应该比较好理解。\n\n## 到底咋办？\n\n最近RESTful真的非常流行，但不要觉得我不用RESTful我就OUT了，我做的这个就不好。两者没有高下之分，只是两种不同的convention，习惯哪个就用哪个就好了，没必要盲目跟风非要用RESTful，又没理解透，最后做出来一个混搭产物REFU（Remove Extension From Url）。\n\n我个人的习惯是用JSON-RPC（现在才知道的词，用了很久都不知道自己的做法叫什么，有点不好意思）。但是RESTful可以有一点参考作用，在做一些静态资源的功能的时候可以考虑使用GET搭配RESTful-like的API作为优化。\n\n另外实际上使用REST的方法也比较简单，在Springboot中，只需要加上`@RestController`和修改`@RequestMapping`即可；在thinkphp中可以通过修改路由的方式实现RESTful。\n\n## 其他参考\n\n[WEB开发中，使用JSON-RPC好，还是RESTful API好？ - 知乎](https://www.zhihu.com/question/28570307)\n\n[HTTP 方法：GET 对比 POST](https://www.w3school.com.cn/tags/html_ref_httpmethods.asp)\n\n[GraphQL](https://graphql.cn/)"},{"url":"/2021/01/09/ThinkPHP6.0 与5.0的差别及坑点/","content":"# ThinkPHP6.0 与5.0的差别及坑点\n\n## 写在前面\n\ntp6发布已经有大半年了，之前做项目一直用的tp5，甚至没有用5.1，是因为tp5用来做了很多个项目，文档前后翻了可能有两三遍，所以对tp5会比较熟悉（个人感觉）。最近刚好做数据库的大作业，时间不大够，放弃了原来用springboot 的打算；正巧又打算把之前的项目从tp5迁移到tp6，所以打算借着这个机会先把tp6的坑踩一下，而且有之前tp5的基础，应该问题不大。\n\n接下来我会按照开发过程的顺序，从控制器（C）、模型（M）、一些配置、乱七八糟会用到的地方进行对比，同时也会讲一些坑点。\n\n## 不同点\n\n### 安装\n\n首先一个比较大的差别是安装不再有分发压缩包的形式，改成了用composer来分发。2020年了，这确实是一个比较好的方案，看得出tp已经是一个比较成熟的框架了。而且同大版本号可以通过composer直接更新，整个流程比较流畅。\n\n食用方式：\n\n```php\ncomposer create-project topthink/think tp\n```\n\n※坑点：composer国内访问较慢，换源可以大大提升体验，方法可以参考https://blog.csdn.net/weixin_43409309/article/details/105577082\n\n### 配置文件\n\n比较明显的是tp5里面的config.php被拆分成了app、cache、database、route等等模块，放在了config文件夹里。这个部分实际上变动不太大，实现也是通过配置加载的方式，我在tp5的时候已经有自己定义配置然后加载了，这个部分没有太大亮点。\n\n值得一提的是，tp6新引入了一个.env的文件，可以通过读取这个环境设置去配置数据库等等之类config里面的内容。文件采用ini格式，可以修改数据库连接等常用的配置，比较方便。之前如果分散在各处的一些配置（如ip地址、密码加密的盐值之类）可以放在同一个地方了，方便了在不同环境里部署。\n\n但是，这里要打一个但是，我一开始以为这里是一个类似springboot里面.properties文件的那种机制，可以通过不同的环境文件区分例如测试环境和开发环境。实际上只能存在一个名叫\".env\"的文件（注意不是后缀名）。这里就会出现一个，在不同环境切换时可能会出现误操作覆盖了这个文件的情况，始终觉得有些别扭。\n\n※坑点：环境文件的命名一定是“.env\"，没有别的字符\n\n### 自带web服务器\n\n这确实是一个比较大的改变，调试的时候再也不需要拿出xampp了，直接`php think run`就可以在本地启动一个web服务器，还可以用`-p`参数指定端口，确实比较方便。我现在已经习惯了在PHPstormProjects下直接开发项目（因为用pycharm用多了），如果要把文件复制到htdocs里面才能测试就很麻烦，这个web服务器有点对我胃口。\n\n※坑点：不知道是什么原因，感觉这个web服务器性能可能有问题，一个普通的响应居然用时500ms+，生产环境下还是用正经的web服务器。\n\n### 目录\n\n新版本的目录是有比较大的变化的，例如之前的config.php就拆分成了一个config文件夹，application文件夹重命名为app文件夹，等等之类。\n\n※坑点：建议看一下tp6文档里的目录结构\n\n### 控制器\n\n这个方面变化还挺大，感觉底层可能有比较大的变动。以往控制器需要继承`think\\Controller`类，现在官方有一个`BaseController`，实现的控制器只需要继承这个控制器基础类就可以。（在Model上就没有看到这个变化）\n\n### json接口\n\n刚好说完控制器，就必须得说一下我在做JSON-RPC的时候经常用的一套方法：封装一个`BaseController`，在这里构造用来返回的JSON；然后其他控制器继承这个基类控制器，就可以比较方便地渲染JSON数据返回。现在的workaround跟原来也是类似的，用一个`ResponseController`去继承`BaseController`，然后其他控制器再继承`ResponseController`。\n\n※坑点：这里有一个比较奇怪的改变是，tp5可以在配置文件中设置返回类型为json，这样在返回时只需要返回一个数组，框架就能包装成json返回。在tp6里没有找到这样的配置，好像只能手动`json()`然后再返回。\n\n※坑点：输入的时候也是，以往输入函数可以自动处理json，例如可以通过`input(\"post.\")`获取到post上来的json，然后解析成一个数组；tp6把json转换成一个表单，需要通过类似表单获取的方式访问，体验跟tp5稍有不同。\n\n### 跨域\n\n还记得之前的跨域问题吗？（https://blog.csdn.net/weixin_43409309/article/details/104287425）这个问题在tp6里终于有比较方便的解决方案了，只需要在`middleware.php`中注册`\\think\\middleware\\AllowCrossDomain`就可以搞定跨域了。\n\n### 中间件\n\ntp6改进了中间件，具体的使用可以看一下文档，因为我没有用过 ，这方面没太大感觉。\n\n## 总结\n\n虽然说是一个大版本更新，底层有比较大的改变，实际操作跟tp5差别并不大。有朋友说tp6刚出的时候比较多bug，因此我也是没有第一时间尝新；现在半年多过去，感觉已经能够稳定使用了，在这一两天中也没有发现其他什么大的问题。\n\n另外有一个比较想吐槽的问题是，tp6的文档更新得真的太慢，很多文档目前还欠缺，例如视图层的文档就直接叫你参考tp5的了，以及tp5的完全开发手册其实也还有很多坑没有填上，而且在说明上也不够清楚。另外，助手函数在附录中单列了出来，但又没有具体的文档，只有一个名录，这方面有点拉胯。希望团队能重视一下文档的建设，像我这样的菜鸡在学习起来会比较有困难。"},{"url":"/2021/01/09/thinkphp5悲观锁机制处理高并发/","content":"# thinkphp悲观锁机制处理高并发\n\n## 问题分析\n\n突然间被运营滴滴说某个活动的报名人数超过了限制人数，问怎么回事，我一下子还挺蒙的，我明明有在报名的操作之前设置了检查如果超过报名人数代码逻辑会抛错继续报名的呀。\n\n![报名人数超过限制的人数](http://img.ameow.xyz/20200702232803.jpg)\n\n然后我又打开数据库看了一下，出现了以下的情况：\n\n![表里有重复插入的数据](http://img.ameow.xyz/20200703001725.png)\n\n于是情况就很明了了，这明显就是并发控制没有做好。为了叙述清楚这个情况，下面讲述一下业务逻辑：首先是从meeting表查是否报名已满，如果未满，则开始事务，将signed字段自增1，然后把参会记录插入到meeting_member表，提交事务。这里实际上是出现了丢失更新，举例如下。\n\n| T1                                                           | T2                                                           | 数据库中signed的值 |\n| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------ |\n| BEGIN;                                                       |                                                              | 0                  |\n| SELECT signed FROM meeting WHERE meeting_id=xx; **（读出来的值为0）** | BEGIN;                                                       |                    |\n| UPDATE meeting SET signed=signed+1 WHERE meeting_id=xx;      | SELECT signed FROM meeting WHERE meeting_id=xx; **（读出来的值也为0）** |                    |\n| COMMIT;                                                      | UPDATE meeting SET signed=signed+1 WHERE meeting_id=xx;      |                    |\n|                                                              | COMMIT;                                                      | 1                  |\n\n可以看到，如果T1没有提交，T2会读取到原来的值，最终出现T1的更新丢失的问题。对应到具体场景就是，如果有两个事务，第一个读取、然后更新，但还没有提交，这时候开始了第二个事务，他读取到的就是第一个事务更新前的数据，同样进行自增，这是T1提交，T2也提交，但是signed只增加了1。\n\n（中间有进行实验，但是由于填这个坑花的时间太长，就不把实验过程放上来了，结果跟上表中罗列的一致）\n\n### PS\n\n一开始我马上联想到的是事务的隔离级别以及脏读、不可重复读、幻读之类的问题，实际上这里出现的是**第二类丢失更新**[<sup>1</sup>]问题，丢失更新是指两个事务更新数据时可能会覆盖对方的更新，并不是脏读（T2错误读取到T1已经修改但未提交的数据）、不可重复读（T1进行两次读，中间T2对数据进行修改并提交，T1两次读到的数据不同）、幻读（T1修改了表中符合某种条件的数据，T2又新增了一条符合这种条件的数据，T1会发现还有没有修改的数据）[<sup>2</sup>]。这就是为什么即使mysql已经是可重复读（Repeatable Read）的事务隔离等级，但还是会出现丢失更新的原因[<sup>3</sup>]。\n\n这里蛮坑的，我往事务隔离等级这个方向想了很久，才发现方向根本不对，可重复读已经解决了脏读和不可重复读的问题[<sup>4</sup>]，问题不是出在事务隔离等级上，而是应该在这里加上一个锁的机制。这里又有新的疑惑，为什么有了事务（锁协议是事务的一种实现方式），还需要另外的锁呢？这里考虑到粒度的问题[<sup>5</sup>]。因此最后应该使用的解决方法是悲观锁或者乐观锁。\n\n## 解决思路\n\n解决方法实际上比较~~简单~~（简单个屁），只需要加上悲观锁或者乐观锁就可以了。值得一提的是，其实把事务隔离等级调成未提交读或者可串行化也能解决问题，但如果使用未提交读那会是一个倒退，可串行化会造成并发性能的严重下降，所以不采用。\n\n如果对比乐观锁和悲观锁，乐观锁需要代码实现（增加一个版本字段），而悲观锁可以用数据库原生的方式实现。悲观锁实现较为简单，但悲观锁的并发性能不如乐观锁。\n\n## 悲观锁介绍\n\n悲观的意思是，每次获取数据的时候，都会担心数据被修改，所以每次获取数据的时候都会进行加锁。在当前场景下，悲观锁就是在读取目前的signed时，给这个数据加上**行级的排他锁**，然后再进行更新。如果在T1还没有提交时，T2（一个同样步骤的事务）想要读取这个数据，因为他想要获得的是一个排它锁，由于T1还未提交，T1持有的排它锁会阻塞T2，T2只能等待T1提交之后才能读这一个数据。\n\n### 排他锁\n\n在mysql中，**排他锁**可以这样使用：\n\n```\nSELECT ... FOR UPDATE;\n```\n\nmysql会对查询结果集中每行都添加排它锁，在事务操作中，更新和删除操作会自动加上排他锁[<sup>7</sup>]。\n\n如果数据被加上了排他锁，那么如果查询中无论是请求共享锁或是排他锁，都会因为之前的排他锁而被阻塞，直到之前的排他锁因为事务提交或回滚释放。如果不带任何锁的SELECT语句，无论查询的数据是否有被加锁（包括共享锁和排他锁），都能够进行查询而不被阻塞。具体的表现可以见[<sup>8</sup>]。\n\n### 行锁和表锁\n\nmysql对表的锁有两种不同的粒度，分别是表锁和行锁。行锁是粒度最小的锁，InnoDB支持行锁和表锁，而MyISAM只支持表锁。这里特意提出来，是因为并不是`SELECT ...  FOR UPDATE;`就是行级锁，只有查询到数据、根据主键和/或对非主键含索引进行查询时才能使用行级锁，其他情况使用的还是表锁。具体原因是，InnoDB行锁是通过对索引的索引项加锁来实现的，这点值得注意。例如这里要实现行锁，就要对这个字段建索引。\n\n### 实验\n\n讲了那么多，做一个实验验证一下。\n\n首先看一下mysql8.0默认的事务隔离级别。（注意8.0与5.x有分别）\n\n```mysql\nselect @@global.transaction_isolation,@@transaction_isolation;\n```\n可以看到默认的事务隔离级别是Repeatable Read（可重复读）。\n\n![](http://img.ameow.xyz/20200703081114.png)\n\n然后新开两个查询连接，这里使用university数据库为例（上课用惯了），事务里进行一个查询和更新操作，如下所示。\n\n```mysql\nBEGIN;\nSELECT * FROM instructor WHERE name='Srinivasan' FOR UPDATE;\nUPDATE instructor SET salary=65001 WHERE ID=10101;\n-- ROLLBACK;\nCOMMIT;\n```\n\n先在第一个连接里执行前两行，开始一个事务T1，然后进行查询并加锁。注意这里对name这一列进行了索引，所以可以实现行级的锁。可以看到T1能够正常进行查询。\n\n![](http://img.ameow.xyz/20200703081509.png)\n\n然后用另一个连接也开始一个事务，对这一个数据进行查询。可以看到查询被阻塞了（没有结果返回）。（实际上，等待一段时间（默认50s）后，就会出现当前会话锁等待超时）\n\n![](http://img.ameow.xyz/20200703081641.png)\n\n回到第一个窗口继续第一个事务的更新操作，这时候第二个事务中的查询和更新操作继续被阻塞。如果第二个事务查询另一行的数据，则不会被阻塞。\n\n![](http://img.ameow.xyz/20200703082835.png)\n\n当第一个事务提交或者回滚时，锁被释放，第二个事务马上可以进行查询和更新操作。\n\n![](http://img.ameow.xyz/20200703082919.png)\n\n## thinkphp5.0实现\n\n讲到这么久，终于步入正题。这里也是有一点感慨，实现就两行代码，实际考虑的东西、涉及到的内容远远不止这么点。\n\n在tp5的用法中比较简单，只需要在链式查询中加入`lock(true)`就可以。具体代码如：\n\n```php\nDb::name('user')->where('id',1)->lock(true)->find();\n//指定使用共享锁\nDb::name('user')->where('id',1)->lock('lock in share mode')->find();\n```\n\n在模型中也可用，用法同数据库方法。\n\n\n\n### 坑点\n\n文档中有提到[<sup>9</sup>]\n\n> 就会自动在生成的SQL语句最后加上 `FOR UPDATE`或者`FOR UPDATE NOWAIT`（Oracle数据库）。\n\n说明实现的方法就是加上`FOR UPDATE`，但没说明是行锁还是表锁，也没有说明要先开启事务才能使用，如果对数据库不够熟悉（例如一年前的我）看到这里就会一脸懵逼。另外，据闻tp6已经实现了乐观锁（？），同时，tp5也可以通过traits来实现乐观锁。\n\n## 参考\n\n[1]: https://blog.csdn.net/paopaopotter/article/details/79259686\t\"数据库第一类第二类丢失更新\"\n[2]: https://blog.csdn.net/yishizuofei/article/details/79453588\t\"脏读、丢失更新、不可重复读、幻读\"\n\n[3]: https://zhuanlan.zhihu.com/p/67210493\t\"Mysql RR级别依然可能丢失更新数据\"\n[4]: https://www.cnblogs.com/zhoujinyi/p/3437475.HTML\t\"MySQL 四种事务隔离级的说明\"\n[5]: https://blog.csdn.net/Scrat_Kong/article/details/84454519\t\"为什么有了事务还需要乐观锁和悲观锁？\"\n[6]: https://crossoverjie.top/2017/07/09/SSM15/\t\"SSM(十五) 乐观锁与悲观锁的实际应用\"\n\n[7]: https://blog.csdn.net/tigernorth/article/details/7948539\t\"MySQL锁的用法之行级锁\"\n\n[8]: https://blog.csdn.net/She_lock/article/details/82022431\t\"mysql读锁（共享锁）与写锁（排他锁）\"\n[9]: https://www.kancloud.cn/manual/thinkphp5/118086\t\"ThinkPHP5.0完全开发手册-lock\"\n\n"},{"url":"/2021/01/09/一个搜索框的故事/","content":"# 搜索框的数据库实现思路和分析\n\n最近遇到了一个问题：搜索框怎么实现？具体来说，数据库查询层面怎么实现？恰好最近在做的项目中也有这个功能，于是进行了一些探索和尝试。\n\n## 功能分析\n\n搜索框一般是对一个或多个字段进行模糊/精确匹配的一个功能。最简单的搜索框是对一个字段进行精确匹配，直接**WHERE**然后**=**就行了，要实现模糊匹配，可以把=换成LIKE，然后把条件加上通配符%。这个方法同样适用于下拉框（性质和搜索是一样的，只是前端已经预设好查询的内容）。\n\n比较复杂但也是比较常见的搜索框，一般是多个字段的模糊/精确匹配。这篇文章主要讨论的就是这种比较复杂的搜索框。从一个字段延伸到多个字段，最显而易见的办法就是用**OR**把各个字段连接起来，用=/LIKE作判断。\n\n## （笨方法）想当然法\n\n### 思路\n\n想当然法是单个搜索框的延伸，也是我最开始使用的方法。用一个表（可以把它叫做搜索表），把需要搜索的信息（字段）全部放在一起。查询的时候用=/LIKE作判断，用OR把各个字段连接起来。\n\n### 实现\n\nSQL\n\n```sql\nSELECT * FROM student\nWHERE\nname LIKE '%2018100%' OR\nstudent_number LIKE '%2018100%' OR\nid_card LIKE '%2018100%' OR\nphone LIKE '%2018100%' OR\ndorm LIKE '%2018100%'\n```\n\nJava\n\n（最后加了一个LIMIT做分页）\n\n```java\npublic String search(String content, String college, int page, int pageSize) {\n    \n    String sql = \"SELECT name,student_number,college,major FROM student_info_2019\";\n    if (!\"\".equals(college) || !\"\".equals(content)) {\n        sql += \" WHERE\";\n    }\n    if (!\"\".equals(college)) {\n        sql += \" college = '\" + college + \"'\";\n        if (!\"\".equals(content)) {\n            sql += \" AND\";\n        }\n    }\n    if (!\"\".equals(content)) {\n        sql += \" name LIKE '\" + content + \"' OR student_number = '\" + content + \"' OR id_card = '\"\n                + content + \"' OR phone = '\" + content + \"' OR dorm = '\" + content + \"'\";\n    }\n    sql += \" LIMIT \" + page * pageSize + \" ,\" + pageSize + \";\";\n    return sql;\n}\n```\n\n### 分析\n\n先看看性能。\n\n（以下测试使用的都是2w行的学生信息数据，查询时间均使用三次取平均的方式计算。）\n\n| 1     | 2     | 3     | 平均  |\n| ----- | ----- | ----- | ----- |\n| 0.072 | 0.068 | 0.068 | 0.069 |\n\n查询时间是可接受的范围。但是有一个问题，这个表不符合第一范式，也就是说有太多冗余的字段。\n\n那么是不是还可以其他的表按照第一范式设计，但专门开设一个搜索表，用来存储搜索的信息呢？（甚至还有一个好处，输出的时候不需要再对字段进行处理）所以来到第二个办法。\n\n## （笨方法改进）独立设搜索表\n\n### 思路\n\n专门开设一个搜索表来存储要搜索的信息，但需要与其他几个对应表保持同步。这里同步可以在对其他几个对应的表进行修改时同时修改搜索表中的内容（再执行一次查询）也可以通过触发器来保持同步。\n\n因为这种方法实在太笨而且繁琐，就不实现了。\n\n### 分析\n\n这个方法会有严重的性能问题（尤其是高并发的时候），最致命的地方就是需要多一次查询来进行同步。同时这个表是完完全全的冗余，数据量大时存储空间的开销十分可观。\n\n另外，在查询时，如果字段数量很多，可能需要写很多的OR，另外需要给所有这些字段建立索引。这时候考虑到，可以引进全文搜索。\n\n## （使用全文检索）独立设搜索表\n\n### 思路\n\n通过恰当地建立索引，可能可以有效提高查询的效率。\n\n### 实现\n\n需要注意的是，5.7之前的mysql只有MyISAM引擎可以使用全文检索，而MyISAM引擎不支持事务（transaction）；5.7之后的mysql的Innodb引擎也可以使用全文检索了。\n\n执行查询前需要添加索引，字段选择需要搜索的字段，索引类型选择FULLTEXT。\n\n```sql\nSELECT * FROM student\nWHERE MATCH (name,student_number,id_card,phone,dorm) AGAINST ('2018100xxxx')\n```\n\n（敏感信息打码处理）\n\n### 分析\n\n性能上确实比前面的方法有所提升（事实上是所有方法中性能最高的）。\n\n| 1     | 2     | 3     | 平均  |\n| ----- | ----- | ----- | ----- |\n| 0.044 | 0.032 | 0.033 | 0.036 |\n\n然而这个方法最后实现的是伪模糊查询，只能实现字段的模糊，不能实现搜索内容的模糊（即只能对搜索内容进行精确匹配，如2018100abcd精确匹配2018100abcd而不能通过2018100匹配到所有的2018100xxxx）。\n\n还有另外一个致命问题是，全文检索对中文的支持不是太好，会出现问题。如果有搜索中文的需求，全文检索不太适合。\n\n## （最终采用）无痛法\n\n### 思路\n\n经过前面那些改进，基本确定了一个完美（伪）的方法必须满足下面的要求\n\n1. 满足第一范式（不能有冗余）\n2. 查询效率要高（尽量接近全文检索）\n3. 能实现（真）模糊查询\n4. 对中文支持要好\n5. 写起来要方便优雅（不需要写一大堆OR）\n\n所以把这个表拆分成了两个，查询时使用连接，同时优化WHERE语句。\n\n### 实现\n\nSQL\n\n```sql\nSELECT * \nFROM student s\nJOIN student_info si ON s.student_number=si.student_number\nWHERE CONCAT(s.name,s.student_number,phone,id_card) LIKE '%2018100%'\n```\n\nJava\n\n```java\npublic String search(String content, String college, int page, int pageSize) {\n            StringBuilder sql = new StringBuilder();\n            sql.append(\"SELECT * FROM student s \");\n            sql.append(\"JOIN student_info si ON s.student_number=si.student_number\");\n            if (!\"\".equals(college) || !\"\".equals(content)) {\n                sql.append(\" WHERE\");\n            }\n            if (!\"\".equals(college)) {\n                sql.append(\" college='\").append(college).append(\"'\");\n                if (!\"\".equals(content)) {\n                    sql.append(\" AND\");\n                }\n            }\n            if (!\"\".equals(content)) {\n                sql.append(\" CONCAT(s.name,s.student_number,phone,id_card) LIKE '%\").append(content).append(\"%'\");\n            }\n            sql.append(\" LIMIT \").append(page * pageSize).append(\" ,\").append(pageSize).append(\";\");\n            return sql.toString();\n        }\n```\n\nps. 这里用StringBuilder而不是String的原因是StringBuilder的性能比String高（String每次操作都会新建对象），但StringBuilder不是线程安全的，在多线程环境下应使用StringBuffer。\n\n### 分析\n\n还是先看看性能。（这里顺便与使用了CONCAT之后的“笨方法”比较）\n\n| 方法   | 1     | 2     | 3     | 平均  |\n| ------ | ----- | ----- | ----- | ----- |\n| 笨方法 | 0.065 | 0.065 | 0.066 | 0.065 |\n| 新方法 | 0.084 | 0.089 | 0.086 | 0.086 |\n\n竟然还是笨方法好？这里提出一个猜测，前面的一堆OR在实际执行查询的时候是被优化了，而使用CONCAT可能就是之前执行的优化，所以使用CONCAT之后对比OR有一点点的提升。\n\n当然这个最终采用的方法不会产生冗余，查询性能相差也不大，所以才最终采用！\n\n## 总结\n\n虽然最终采用的是多表连接、优化WHERE的查询方法，但是单从性能上说，单表全文索引>单表模糊>多表连接。全文索引对中文支持差、不能实现真模糊、同样存在单独搜索表的问题，但性能明显翻倍。单独搜索表可以实现真模糊、性能也适中，但是冗余开销较大、需要解决同步问题。多表连接是最无痛的方法，但是性能偏低。但当前这个项目对高并发没有要求，也不要求很高的反应速度，因此使用了比较规范的无痛方法。"},{"title":"Hello World","url":"/2019/12/30/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"},{"title":"JWT详解和使用（jjwt）","url":"/2019/10/15/JWT详解和使用（jjwt）/","content":"\n# JWT详解和使用\n\n## JWT是啥\n\nJWT（JSON Web Token）是一个开放标准(RFC 7519)，它定义了一种紧凑的、自包含的方式，用于作为JSON对象在各方之间安全地传输信息。该信息可以被验证和信任，因为它是数字签名的。\n\n下列场景中使用JSON Web Token是很有用的：\n\n- **Authorization** (授权) : 这是使用JWT的最常见场景。一旦用户登录，后续每个请求都将包含JWT，允许用户访问该令牌允许的路由、服务和资源。**单点登录**是现在广泛使用的JWT的一个特性，因为它的开销很小，并且可以轻松地**跨域**使用。\n- **Information Exchange** (信息交换) : 对于安全的在各方之间传输信息而言，JSON Web Tokens无疑是一种很好的方式。因为JWTs可以被签名，例如，用公钥/私钥对，你可以确定发送人就是它们所说的那个人。另外，由于签名是使用头和有效负载计算的，您还可以验证内容没有被篡改。\n\n\n\n## JWT怎么用\n\n在登录时，服务器端获取用户的信息等，根据需求（如expiration等）生成一个JWT返回给客户端。客户端每次请求时，带上该JWT，服务器端只需要对该JWT进行解密，就可以得知该JWT是否有效、用户的信息（注意不能有敏感信息）等。这个过程服务端不需要存储JWT的内容（不同于之前类似Session id的那种token），只需要对JWT进行加解密操作。\n\nps.之前我使用的类似Session id的token是指用随机生成的字符串作为token（同时作为缓存的key），将用户信息json化（或其他序列化方式）缓存。token返回给客户端，客户端每次请求时带上token，服务器就可以从缓存中读出用户信息，减少了数据库的压力（？）。\n\n## JWT的结构\n\n![874963-20180709124807031-664967381.png](https://i.loli.net/2019/10/15/EqYNob7VF8tSeGj.png)\n\n如图，JWT由三个部分组成，之间由一个“.”连接\n\n- Header/头部（我来组成头部x）\n- Payload/载荷\n- Signature/签名\n\n下面具体介绍这三个部分\n\n#### Header/头部\n\n头部一般由两个部分组成，token的类型和使用的算法。形式如下：\n\n```json\n{\n\t\"alg\":\"HS256\",\n\t\"typ\":\"JWT\",\n    \"zip\":\"...\",\n    \"YOUR_KEY\":\"YOUR_VALUE\"\n}\n```\n\n当然还可以增加一些如zip（指示压缩方法）等自定义的字段。\n\n\n\n#### Payload（Body）/载荷\n\npayload部分是JWT存储信息的部分，包含着Claims（声明），其实就是存储的的数据。\n\n一般声明分为以下三种类型：\n\n- Registered claims：预定义的声明，如：\n  - **iss**：issuer 发布者的URL地址\n  - sub：subject JWT面向的用户，不常用\n  - aud：audience 接受者的URL地址\n  - **exp**：expiration JWT失效的时间（Unix timestamp）\n  - **nbf**：not before 该时间前JWT无效（Unix timestamp）\n  - **iat**：issued at JWT发布时间（Unix timestamp）\n  - **jti**：JWT ID\n- Public claims：公开字段（也不知道干啥的）\n- **Private claims**：自定义私有字段（可以在这个字段里定义要传递的用户信息等）***不能在此字段传递加密的信息***，这部分采用对称加密方式，传输内容可以被解开。\n\n##### eg.\n\n```json\n{\n    \"sub\":\"1234567890\",\n    \"name\":\"John Doe\",\n    \"admin\":true\n}\n```\n\n\n\n#### Signature/签名\n\n签名部分的生成公式如下\n\n```\nHMACSHA256(base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret)\n```\n\n即base64编码的header和payload，加上一个秘钥。\n\n签名的用途显而易见就是验证前面部分的内容是否有被篡改（因为前面是对称加密的，容易修改）。\n\n\n\n## 使用jjwt生成JWT\n\n这里使用了一个开源项目jjwt（https://github.com/jwtk/jjwt）来实现。当然JWT的实现也不难，这里因为懒（以及菜）就选择用这个开源项目了。\n\n#### Installation/部署\n\n首先需要在Maven中添加dependencies，如下\n\n```xml\n<dependency>\n    <groupId>io.jsonwebtoken</groupId>\n    <artifactId>jjwt-api</artifactId>\n    <version>0.10.7</version>\n</dependency>\n<dependency>\n    <groupId>io.jsonwebtoken</groupId>\n    <artifactId>jjwt-impl</artifactId>\n    <version>0.10.7</version>\n    <scope>runtime</scope>\n</dependency>\n<dependency>\n    <groupId>io.jsonwebtoken</groupId>\n    <artifactId>jjwt-jackson</artifactId>\n    <version>0.10.7</version>\n    <scope>runtime</scope>\n</dependency>\n```\n\n我使用的环境是IDEA，设置了Auto-import，所以添加了之后稍等一下就发现红线消失，依赖问题解决了。\n\nMaven真好用 IDEA真聪明（小声逼逼\n\n\n\n#### Quickstart/快车\n\n一个JWS（signed JWT）的创建过程大致分为以下三步：\n\n1. *构建*一个有默认载荷的JWT\n2. 用秘钥*签名*这个JWT（秘钥必须满足HMAC-SHA-256算法）\n3. *打包*JWT成一个字符串\n\n贴一下代码\n\n```java\nimport io.jsonwebtoken.Jwts;\nimport io.jsonwebtoken.SignatureAlgorithm;\nimport io.jsonwebtoken.security.Keys;\nimport java.security.Key;\n\n// We need a signing key, so we'll create one just for this example. Usually\n// the key would be read from your application configuration instead.\nKey key = Keys.secretKeyFor(SignatureAlgorithm.HS256);\n\nString jws = Jwts.builder().setSubject(\"Joe\").signWith(key).compact();\n```\n\n注意到第8行，这里用助手函数生成了一个随机的满足加密算法要求的key，在正常使用中，需要自己定义一个秘钥。\n\n输出会得到一个类似如下的字符串，这个就是生成的JWT，可以看到Header、Payload和Signature三个部分\n\n```\neyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKb2UifQ.1KP0SsvENi7Uz1oQc07aXTL7kpQG5jBNIybqr60AlD4\n```\n\n那么如何验证这个JWT是否有效呢\n\n贴代码\n\n```java\ntry {\n\n    Jwts.parser().setSigningKey(key).parseClaimsJws(compactJws);\n\n    //OK, we can trust this JWT\n\n} catch (JwtException e) {\n\n    //don't trust the JWT!\n}\n```\n\nOK，JWT生成、验证的过程就这么简单！\n\n\n\n接下来开始正式使用部分的介绍\n\n#### 生成JWT\n\n先贴代码\n\n```java\n        byte[] secret = \"2162d3e65a421bc0ac76ae5acfe29c650becb73f2a9b8ce57941036331b1aaa8\".getBytes();\n        SecretKey key = Keys.hmacShaKeyFor(secret);\n        \n        String jws = Jwts.builder()\n                .setHeaderParam(\"kid\", \"123456\")\n                .setSubject(\"111\")\n                .setIssuer(\"ameow\")\n                .setNotBefore(new Date())\n                .claim(\"weisha\", \"wozhidaole\")\n                .signWith(key)\n                .compact();\n```\n\n首先是我比较关心的key的部分。查阅文档（https://github.com/jwtk/jjwt ）可以知道，JWT对key的长度是有要求的，以这里SHA-256为例，就需要256位的key。具体加密方法对应的key要求可以在文档中查到，不多叙述（因为不会）。\n\n这里我想选择一个字符串“hello”作为key，于是我需要生成一个对应的SHA-256加密的串，这里可以用Java实现SHA-256加密，也可以使用在线的SHA-256生成工具直接生成这个串，然后直接换成byte形式，用jjwt提供的助手方法转换成key。*（可能这样会不太安全？）*\n\n然后就是JWS的部分，用Jwts.builder()，然后就是set里面的内容，最后sign和compact，就可以得到生成的JWT。\n\n如果要生成自定义的claim，可以采用以下的方法\n\n```java\nClaims claims = Jwts.claims();\n\npopulate(claims); //implement me\n\nString jws = Jwts.builder()\n\n    .setClaims(claims)\n    \n    // ... etc ...\n```\n\n其他部分暂时还没有接触到，碰到再研究。\n\n#### 读取JWT\n\n简单几步\n\n1. 用Jwts.parser()创建一个JwtParser对象\n2. 指定要使用的秘钥setSigningKey()\n3. 调用parseClaimsJws()方法\n4. 用try/catch块包裹这部分内容，方便jjwt处理异常\n\n如果要获取里面的内容，可以采用以下代码\n\n```java\nJws<Claims> jwsR;\n        try {\n            jwsR = Jwts.parser()\n                    .setSigningKey(key)\n                    .parseClaimsJws(jws);\n            System.out.println(jwsR);\n            System.out.println(jwsR.getBody().get(\"weisha\"));\n\n        } catch (JwtException ex) {\n            System.out.println(\"???\");\n        }\n```\n\n输出如下（1是传入的JWT，2是JWT解释出的内容，3是获取到的Body中的weisha字段中的内容）\n\n```\neyJraWQiOiIxMjM0NTYiLCJhbGciOiJIUzUxMiJ9.eyJzdWIiOiIxMTEiLCJpc3MiOiJhbWVvdyIsIm5iZiI6MTU3MTEyMjI4MCwid2Vpc2hhIjoid296aGlkYW9sZSJ9.nOvpsOZ93bmdjBgRRADoWJhTAJ-QOrumHtFgqCd6V9CAafZe0nzXCBxbw2YmsVKLW2i2SGy0FbgZjBtt_H8Q3w\nheader={kid=123456, alg=HS512},body={sub=111, iss=ameow, nbf=1571122280, weisha=wozhidaole},signature=nOvpsOZ93bmdjBgRRADoWJhTAJ-QOrumHtFgqCd6V9CAafZe0nzXCBxbw2YmsVKLW2i2SGy0FbgZjBtt_H8Q3w\nwozhidaole\n```\n\n\n\n## 最后\n\n这篇博客没有提到加密方面的细节，是因为我对安全方面不太了解，这方面还有待学习。如果有安全方面需求，可以查阅以下的ref。\n\nRef：\n\nhttps://www.cnblogs.com/cjsblog/p/9277677.html\n\nhttps://github.com/jwtk/jjwt\n\nhttps://jwt.io/","tags":["后台"]},{"title":"TensorFlow解决MNIST数字识别问题","url":"/2019/10/14/TensorFlow解决MNIST数字识别问题/","content":"## 废话\n\n这个MNIST数字识别问题是我实现的第一个神经网络，虽然过程基本上都是对着书上的代码敲，但还是对神经网络的训练过程有了一定的了解，同时也复习了前面几章关于TensorFlow和神经网络的一些基本概念。\n\n## MNIST介绍\n\nMNIST是一个非常有名的手写体数字识别数据集，通常用来作为深度学习的入门样例。\n\nMNIST的数据集可以在[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)下载\n\nTensorFlow提供了一个类来处理MNIST数据，能够自动下载并转化MNIST数据的格式。\n\n\n\n## 训练神经网络\n\n直接先贴代码\n\n```\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# MNIST相关常数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\n\n# 神经网络参数\nLAYER1_NODE = 500\nBATCH_SIZE = 100\nLEARNING_RATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARIZATION_RATE = 0.0001\nTRAINING_STEPS = 30000\nMOVING_AVERAGE_DECAY = 0.99\n\n\ndef inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n    if avg_class == None:\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n        return tf.matmul(layer1, weights2) + biases2\n    else:\n        layer1 = tf.nn.relu(\n            tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1)\n        )\n        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n\n\ndef train(mnist):\n    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n\n    weights1 = tf.Variable(\n        tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n    weights2 = tf.Variable(\n        tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n\n    y = inference(x, None, weights1, biases1, weights2, biases2)\n\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n    regularization = regularizer(weights1) + regularizer(weights2)\n    loss = cross_entropy_mean + regularization\n\n    learning_rate = tf.train.exponential_decay(\n        LEARNING_RATE_BASE, global_step, mnist.train.num_examples, LEARNING_RATE_DECAY\n    )\n\n    train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n        .minimize(loss, global_step=global_step)\n\n    with tf.control_dependencies([train_step, variables_averages_op]):\n        train_op = tf.no_op(name='train')\n\n    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        validate_feed = {\n            x: mnist.validation.images,\n            y_: mnist.validation.labels\n        }\n\n        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n\n        for i in range(TRAINING_STEPS):\n            if i % 1000 == 0:\n                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n                test_acc = sess.run(accuracy, feed_dict=test_feed)\n                print(\"After %d training step(s), validation accuracy \"\n                      \"using average model is %g , test accuracy is %g\" % (i, validate_acc, test_acc))\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            sess.run(train_op, feed_dict={x: xs, y_: ys})\n\n        test_acc = sess.run(accuracy, feed_dict=test_feed)\n        print(\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc))\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(\"/temp/data\", one_hot=True)\n    train(mnist)\n\n\nif __name__ == '__main__':\n    tf.app.run()\n```\n然后是输出结果\n\n```python\nExtracting /temp/data\\train-images-idx3-ubyte.gz\nWARNING:tensorflow:From C:/Users/lesil/PycharmProjects/matchzoo/MNIST.py:93: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting /temp/data\\train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting /temp/data\\t10k-images-idx3-ubyte.gz\nExtracting /temp/data\\t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n2019-08-11 11:43:46.478172: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\nAfter 0 training step(s), validation accuracy using average model is 0.1596 , test accuracy is 0.1702\nAfter 1000 training step(s), validation accuracy using average model is 0.9766 , test accuracy is 0.975\nAfter 2000 training step(s), validation accuracy using average model is 0.9812 , test accuracy is 0.9809\nAfter 3000 training step(s), validation accuracy using average model is 0.9828 , test accuracy is 0.9828\nAfter 4000 training step(s), validation accuracy using average model is 0.9836 , test accuracy is 0.9837\nAfter 5000 training step(s), validation accuracy using average model is 0.9834 , test accuracy is 0.9835\nAfter 6000 training step(s), validation accuracy using average model is 0.985 , test accuracy is 0.985\nAfter 7000 training step(s), validation accuracy using average model is 0.9846 , test accuracy is 0.9845\nAfter 8000 training step(s), validation accuracy using average model is 0.9852 , test accuracy is 0.9842\nAfter 9000 training step(s), validation accuracy using average model is 0.9844 , test accuracy is 0.9852\nAfter 10000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9844\nAfter 11000 training step(s), validation accuracy using average model is 0.9854 , test accuracy is 0.9845\nAfter 12000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.984\nAfter 13000 training step(s), validation accuracy using average model is 0.9844 , test accuracy is 0.984\nAfter 14000 training step(s), validation accuracy using average model is 0.9854 , test accuracy is 0.9842\nAfter 15000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9842\nAfter 16000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9841\nAfter 17000 training step(s), validation accuracy using average model is 0.9856 , test accuracy is 0.9838\nAfter 18000 training step(s), validation accuracy using average model is 0.9848 , test accuracy is 0.9848\nAfter 19000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9835\nAfter 20000 training step(s), validation accuracy using average model is 0.9864 , test accuracy is 0.9844\nAfter 21000 training step(s), validation accuracy using average model is 0.9868 , test accuracy is 0.9845\nAfter 22000 training step(s), validation accuracy using average model is 0.9856 , test accuracy is 0.9844\nAfter 23000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9842\nAfter 24000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9845\nAfter 25000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9845\nAfter 26000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9843\nAfter 27000 training step(s), validation accuracy using average model is 0.9864 , test accuracy is 0.984\nAfter 28000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9843\nAfter 29000 training step(s), validation accuracy using average model is 0.9864 , test accuracy is 0.9842\nAfter 30000 training step(s), test accuracy using average model is 0.9846\n\nProcess finished with exit code 0\n```\n## 几个坑点\n\n* 书上的代码有部分缩进错误，在python中缩进错误是直接gg的。在这里要通过看训练的过程（也就是train函数的部分）纠正一下原本的缩进错误。\n* 在使用L2正则化损失函数时，注意是l2而不是12，因为这里ide没有补全提示_​_**_（为什么？）_****​**​比较容易出现typo。\n\n\n\n## 训练过程\n\n**​一轮训练的过程**\n\n首先计算当前参数下神经网络前向传播的结果，然后在所有代表神经网络参数的变量上使用滑动平均，然后计算使用了滑动平均之后的前向传播\n\n\n","tags":["NLP"]},{"title":"meow","url":"/2019/10/14/meow/","content":"\n我们一起喵喵喵\n\n"}]