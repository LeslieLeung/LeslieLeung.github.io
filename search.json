[{"title":"Hello World","url":"/2019/12/30/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"},{"title":"JWT详解和使用（jjwt）","url":"/2019/10/15/JWT详解和使用（jjwt）/","content":"\n# JWT详解和使用\n\n## JWT是啥\n\nJWT（JSON Web Token）是一个开放标准(RFC 7519)，它定义了一种紧凑的、自包含的方式，用于作为JSON对象在各方之间安全地传输信息。该信息可以被验证和信任，因为它是数字签名的。\n\n下列场景中使用JSON Web Token是很有用的：\n\n- **Authorization** (授权) : 这是使用JWT的最常见场景。一旦用户登录，后续每个请求都将包含JWT，允许用户访问该令牌允许的路由、服务和资源。**单点登录**是现在广泛使用的JWT的一个特性，因为它的开销很小，并且可以轻松地**跨域**使用。\n- **Information Exchange** (信息交换) : 对于安全的在各方之间传输信息而言，JSON Web Tokens无疑是一种很好的方式。因为JWTs可以被签名，例如，用公钥/私钥对，你可以确定发送人就是它们所说的那个人。另外，由于签名是使用头和有效负载计算的，您还可以验证内容没有被篡改。\n\n\n\n## JWT怎么用\n\n在登录时，服务器端获取用户的信息等，根据需求（如expiration等）生成一个JWT返回给客户端。客户端每次请求时，带上该JWT，服务器端只需要对该JWT进行解密，就可以得知该JWT是否有效、用户的信息（注意不能有敏感信息）等。这个过程服务端不需要存储JWT的内容（不同于之前类似Session id的那种token），只需要对JWT进行加解密操作。\n\nps.之前我使用的类似Session id的token是指用随机生成的字符串作为token（同时作为缓存的key），将用户信息json化（或其他序列化方式）缓存。token返回给客户端，客户端每次请求时带上token，服务器就可以从缓存中读出用户信息，减少了数据库的压力（？）。\n\n## JWT的结构\n\n![874963-20180709124807031-664967381.png](https://i.loli.net/2019/10/15/EqYNob7VF8tSeGj.png)\n\n如图，JWT由三个部分组成，之间由一个“.”连接\n\n- Header/头部（我来组成头部x）\n- Payload/载荷\n- Signature/签名\n\n下面具体介绍这三个部分\n\n#### Header/头部\n\n头部一般由两个部分组成，token的类型和使用的算法。形式如下：\n\n```json\n{\n\t\"alg\":\"HS256\",\n\t\"typ\":\"JWT\",\n    \"zip\":\"...\",\n    \"YOUR_KEY\":\"YOUR_VALUE\"\n}\n```\n\n当然还可以增加一些如zip（指示压缩方法）等自定义的字段。\n\n\n\n#### Payload（Body）/载荷\n\npayload部分是JWT存储信息的部分，包含着Claims（声明），其实就是存储的的数据。\n\n一般声明分为以下三种类型：\n\n- Registered claims：预定义的声明，如：\n  - **iss**：issuer 发布者的URL地址\n  - sub：subject JWT面向的用户，不常用\n  - aud：audience 接受者的URL地址\n  - **exp**：expiration JWT失效的时间（Unix timestamp）\n  - **nbf**：not before 该时间前JWT无效（Unix timestamp）\n  - **iat**：issued at JWT发布时间（Unix timestamp）\n  - **jti**：JWT ID\n- Public claims：公开字段（也不知道干啥的）\n- **Private claims**：自定义私有字段（可以在这个字段里定义要传递的用户信息等）***不能在此字段传递加密的信息***，这部分采用对称加密方式，传输内容可以被解开。\n\n##### eg.\n\n```json\n{\n    \"sub\":\"1234567890\",\n    \"name\":\"John Doe\",\n    \"admin\":true\n}\n```\n\n\n\n#### Signature/签名\n\n签名部分的生成公式如下\n\n```\nHMACSHA256(base64UrlEncode(header) + \".\" + base64UrlEncode(payload), secret)\n```\n\n即base64编码的header和payload，加上一个秘钥。\n\n签名的用途显而易见就是验证前面部分的内容是否有被篡改（因为前面是对称加密的，容易修改）。\n\n\n\n## 使用jjwt生成JWT\n\n这里使用了一个开源项目jjwt（https://github.com/jwtk/jjwt）来实现。当然JWT的实现也不难，这里因为懒（以及菜）就选择用这个开源项目了。\n\n#### Installation/部署\n\n首先需要在Maven中添加dependencies，如下\n\n```xml\n<dependency>\n    <groupId>io.jsonwebtoken</groupId>\n    <artifactId>jjwt-api</artifactId>\n    <version>0.10.7</version>\n</dependency>\n<dependency>\n    <groupId>io.jsonwebtoken</groupId>\n    <artifactId>jjwt-impl</artifactId>\n    <version>0.10.7</version>\n    <scope>runtime</scope>\n</dependency>\n<dependency>\n    <groupId>io.jsonwebtoken</groupId>\n    <artifactId>jjwt-jackson</artifactId>\n    <version>0.10.7</version>\n    <scope>runtime</scope>\n</dependency>\n```\n\n我使用的环境是IDEA，设置了Auto-import，所以添加了之后稍等一下就发现红线消失，依赖问题解决了。\n\nMaven真好用 IDEA真聪明（小声逼逼\n\n\n\n#### Quickstart/快车\n\n一个JWS（signed JWT）的创建过程大致分为以下三步：\n\n1. *构建*一个有默认载荷的JWT\n2. 用秘钥*签名*这个JWT（秘钥必须满足HMAC-SHA-256算法）\n3. *打包*JWT成一个字符串\n\n贴一下代码\n\n```java\nimport io.jsonwebtoken.Jwts;\nimport io.jsonwebtoken.SignatureAlgorithm;\nimport io.jsonwebtoken.security.Keys;\nimport java.security.Key;\n\n// We need a signing key, so we'll create one just for this example. Usually\n// the key would be read from your application configuration instead.\nKey key = Keys.secretKeyFor(SignatureAlgorithm.HS256);\n\nString jws = Jwts.builder().setSubject(\"Joe\").signWith(key).compact();\n```\n\n注意到第8行，这里用助手函数生成了一个随机的满足加密算法要求的key，在正常使用中，需要自己定义一个秘钥。\n\n输出会得到一个类似如下的字符串，这个就是生成的JWT，可以看到Header、Payload和Signature三个部分\n\n```\neyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJKb2UifQ.1KP0SsvENi7Uz1oQc07aXTL7kpQG5jBNIybqr60AlD4\n```\n\n那么如何验证这个JWT是否有效呢\n\n贴代码\n\n```java\ntry {\n\n    Jwts.parser().setSigningKey(key).parseClaimsJws(compactJws);\n\n    //OK, we can trust this JWT\n\n} catch (JwtException e) {\n\n    //don't trust the JWT!\n}\n```\n\nOK，JWT生成、验证的过程就这么简单！\n\n\n\n接下来开始正式使用部分的介绍\n\n#### 生成JWT\n\n先贴代码\n\n```java\n        byte[] secret = \"2162d3e65a421bc0ac76ae5acfe29c650becb73f2a9b8ce57941036331b1aaa8\".getBytes();\n        SecretKey key = Keys.hmacShaKeyFor(secret);\n        \n        String jws = Jwts.builder()\n                .setHeaderParam(\"kid\", \"123456\")\n                .setSubject(\"111\")\n                .setIssuer(\"ameow\")\n                .setNotBefore(new Date())\n                .claim(\"weisha\", \"wozhidaole\")\n                .signWith(key)\n                .compact();\n```\n\n首先是我比较关心的key的部分。查阅文档（https://github.com/jwtk/jjwt ）可以知道，JWT对key的长度是有要求的，以这里SHA-256为例，就需要256位的key。具体加密方法对应的key要求可以在文档中查到，不多叙述（因为不会）。\n\n这里我想选择一个字符串“hello”作为key，于是我需要生成一个对应的SHA-256加密的串，这里可以用Java实现SHA-256加密，也可以使用在线的SHA-256生成工具直接生成这个串，然后直接换成byte形式，用jjwt提供的助手方法转换成key。*（可能这样会不太安全？）*\n\n然后就是JWS的部分，用Jwts.builder()，然后就是set里面的内容，最后sign和compact，就可以得到生成的JWT。\n\n如果要生成自定义的claim，可以采用以下的方法\n\n```java\nClaims claims = Jwts.claims();\n\npopulate(claims); //implement me\n\nString jws = Jwts.builder()\n\n    .setClaims(claims)\n    \n    // ... etc ...\n```\n\n其他部分暂时还没有接触到，碰到再研究。\n\n#### 读取JWT\n\n简单几步\n\n1. 用Jwts.parser()创建一个JwtParser对象\n2. 指定要使用的秘钥setSigningKey()\n3. 调用parseClaimsJws()方法\n4. 用try/catch块包裹这部分内容，方便jjwt处理异常\n\n如果要获取里面的内容，可以采用以下代码\n\n```java\nJws<Claims> jwsR;\n        try {\n            jwsR = Jwts.parser()\n                    .setSigningKey(key)\n                    .parseClaimsJws(jws);\n            System.out.println(jwsR);\n            System.out.println(jwsR.getBody().get(\"weisha\"));\n\n        } catch (JwtException ex) {\n            System.out.println(\"???\");\n        }\n```\n\n输出如下（1是传入的JWT，2是JWT解释出的内容，3是获取到的Body中的weisha字段中的内容）\n\n```\neyJraWQiOiIxMjM0NTYiLCJhbGciOiJIUzUxMiJ9.eyJzdWIiOiIxMTEiLCJpc3MiOiJhbWVvdyIsIm5iZiI6MTU3MTEyMjI4MCwid2Vpc2hhIjoid296aGlkYW9sZSJ9.nOvpsOZ93bmdjBgRRADoWJhTAJ-QOrumHtFgqCd6V9CAafZe0nzXCBxbw2YmsVKLW2i2SGy0FbgZjBtt_H8Q3w\nheader={kid=123456, alg=HS512},body={sub=111, iss=ameow, nbf=1571122280, weisha=wozhidaole},signature=nOvpsOZ93bmdjBgRRADoWJhTAJ-QOrumHtFgqCd6V9CAafZe0nzXCBxbw2YmsVKLW2i2SGy0FbgZjBtt_H8Q3w\nwozhidaole\n```\n\n\n\n## 最后\n\n这篇博客没有提到加密方面的细节，是因为我对安全方面不太了解，这方面还有待学习。如果有安全方面需求，可以查阅以下的ref。\n\nRef：\n\nhttps://www.cnblogs.com/cjsblog/p/9277677.html\n\nhttps://github.com/jwtk/jjwt\n\nhttps://jwt.io/","tags":["后台"]},{"title":"TensorFlow解决MNIST数字识别问题","url":"/2019/10/14/TensorFlow解决MNIST数字识别问题/","content":"## 废话\n\n这个MNIST数字识别问题是我实现的第一个神经网络，虽然过程基本上都是对着书上的代码敲，但还是对神经网络的训练过程有了一定的了解，同时也复习了前面几章关于TensorFlow和神经网络的一些基本概念。\n\n## MNIST介绍\n\nMNIST是一个非常有名的手写体数字识别数据集，通常用来作为深度学习的入门样例。\n\nMNIST的数据集可以在[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)下载\n\nTensorFlow提供了一个类来处理MNIST数据，能够自动下载并转化MNIST数据的格式。\n\n\n\n## 训练神经网络\n\n直接先贴代码\n\n```\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# MNIST相关常数\nINPUT_NODE = 784\nOUTPUT_NODE = 10\n\n# 神经网络参数\nLAYER1_NODE = 500\nBATCH_SIZE = 100\nLEARNING_RATE_BASE = 0.8\nLEARNING_RATE_DECAY = 0.99\nREGULARIZATION_RATE = 0.0001\nTRAINING_STEPS = 30000\nMOVING_AVERAGE_DECAY = 0.99\n\n\ndef inference(input_tensor, avg_class, weights1, biases1, weights2, biases2):\n    if avg_class == None:\n        layer1 = tf.nn.relu(tf.matmul(input_tensor, weights1) + biases1)\n        return tf.matmul(layer1, weights2) + biases2\n    else:\n        layer1 = tf.nn.relu(\n            tf.matmul(input_tensor, avg_class.average(weights1)) + avg_class.average(biases1)\n        )\n        return tf.matmul(layer1, avg_class.average(weights2)) + avg_class.average(biases2)\n\n\ndef train(mnist):\n    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n\n    weights1 = tf.Variable(\n        tf.truncated_normal([INPUT_NODE, LAYER1_NODE], stddev=0.1))\n    biases1 = tf.Variable(tf.constant(0.1, shape=[LAYER1_NODE]))\n    weights2 = tf.Variable(\n        tf.truncated_normal([LAYER1_NODE, OUTPUT_NODE], stddev=0.1))\n    biases2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n\n    y = inference(x, None, weights1, biases1, weights2, biases2)\n\n    global_step = tf.Variable(0, trainable=False)\n\n    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    average_y = inference(x, variable_averages, weights1, biases1, weights2, biases2)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n\n    regularizer = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)\n    regularization = regularizer(weights1) + regularizer(weights2)\n    loss = cross_entropy_mean + regularization\n\n    learning_rate = tf.train.exponential_decay(\n        LEARNING_RATE_BASE, global_step, mnist.train.num_examples, LEARNING_RATE_DECAY\n    )\n\n    train_step = tf.train.GradientDescentOptimizer(learning_rate) \\\n        .minimize(loss, global_step=global_step)\n\n    with tf.control_dependencies([train_step, variables_averages_op]):\n        train_op = tf.no_op(name='train')\n\n    correct_prediction = tf.equal(tf.argmax(average_y, 1), tf.argmax(y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        validate_feed = {\n            x: mnist.validation.images,\n            y_: mnist.validation.labels\n        }\n\n        test_feed = {x: mnist.test.images, y_: mnist.test.labels}\n\n        for i in range(TRAINING_STEPS):\n            if i % 1000 == 0:\n                validate_acc = sess.run(accuracy, feed_dict=validate_feed)\n                test_acc = sess.run(accuracy, feed_dict=test_feed)\n                print(\"After %d training step(s), validation accuracy \"\n                      \"using average model is %g , test accuracy is %g\" % (i, validate_acc, test_acc))\n            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n            sess.run(train_op, feed_dict={x: xs, y_: ys})\n\n        test_acc = sess.run(accuracy, feed_dict=test_feed)\n        print(\"After %d training step(s), test accuracy using average model is %g\" % (TRAINING_STEPS, test_acc))\n\n\ndef main(argv=None):\n    mnist = input_data.read_data_sets(\"/temp/data\", one_hot=True)\n    train(mnist)\n\n\nif __name__ == '__main__':\n    tf.app.run()\n```\n然后是输出结果\n\n```python\nExtracting /temp/data\\train-images-idx3-ubyte.gz\nWARNING:tensorflow:From C:/Users/lesil/PycharmProjects/matchzoo/MNIST.py:93: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting /temp/data\\train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting /temp/data\\t10k-images-idx3-ubyte.gz\nExtracting /temp/data\\t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From C:\\Users\\lesil\\Anaconda3\\envs\\matchzoo\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n2019-08-11 11:43:46.478172: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\nAfter 0 training step(s), validation accuracy using average model is 0.1596 , test accuracy is 0.1702\nAfter 1000 training step(s), validation accuracy using average model is 0.9766 , test accuracy is 0.975\nAfter 2000 training step(s), validation accuracy using average model is 0.9812 , test accuracy is 0.9809\nAfter 3000 training step(s), validation accuracy using average model is 0.9828 , test accuracy is 0.9828\nAfter 4000 training step(s), validation accuracy using average model is 0.9836 , test accuracy is 0.9837\nAfter 5000 training step(s), validation accuracy using average model is 0.9834 , test accuracy is 0.9835\nAfter 6000 training step(s), validation accuracy using average model is 0.985 , test accuracy is 0.985\nAfter 7000 training step(s), validation accuracy using average model is 0.9846 , test accuracy is 0.9845\nAfter 8000 training step(s), validation accuracy using average model is 0.9852 , test accuracy is 0.9842\nAfter 9000 training step(s), validation accuracy using average model is 0.9844 , test accuracy is 0.9852\nAfter 10000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9844\nAfter 11000 training step(s), validation accuracy using average model is 0.9854 , test accuracy is 0.9845\nAfter 12000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.984\nAfter 13000 training step(s), validation accuracy using average model is 0.9844 , test accuracy is 0.984\nAfter 14000 training step(s), validation accuracy using average model is 0.9854 , test accuracy is 0.9842\nAfter 15000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9842\nAfter 16000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9841\nAfter 17000 training step(s), validation accuracy using average model is 0.9856 , test accuracy is 0.9838\nAfter 18000 training step(s), validation accuracy using average model is 0.9848 , test accuracy is 0.9848\nAfter 19000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9835\nAfter 20000 training step(s), validation accuracy using average model is 0.9864 , test accuracy is 0.9844\nAfter 21000 training step(s), validation accuracy using average model is 0.9868 , test accuracy is 0.9845\nAfter 22000 training step(s), validation accuracy using average model is 0.9856 , test accuracy is 0.9844\nAfter 23000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9842\nAfter 24000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9845\nAfter 25000 training step(s), validation accuracy using average model is 0.9862 , test accuracy is 0.9845\nAfter 26000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9843\nAfter 27000 training step(s), validation accuracy using average model is 0.9864 , test accuracy is 0.984\nAfter 28000 training step(s), validation accuracy using average model is 0.9858 , test accuracy is 0.9843\nAfter 29000 training step(s), validation accuracy using average model is 0.9864 , test accuracy is 0.9842\nAfter 30000 training step(s), test accuracy using average model is 0.9846\n\nProcess finished with exit code 0\n```\n## 几个坑点\n\n* 书上的代码有部分缩进错误，在python中缩进错误是直接gg的。在这里要通过看训练的过程（也就是train函数的部分）纠正一下原本的缩进错误。\n* 在使用L2正则化损失函数时，注意是l2而不是12，因为这里ide没有补全提示_​_**_（为什么？）_****​**​比较容易出现typo。\n\n\n\n## 训练过程\n\n**​一轮训练的过程**\n\n首先计算当前参数下神经网络前向传播的结果，然后在所有代表神经网络参数的变量上使用滑动平均，然后计算使用了滑动平均之后的前向传播\n\n\n","tags":["NLP"]},{"title":"meow","url":"/2019/10/14/meow/","content":"\n我们一起喵喵喵\n\n"}]